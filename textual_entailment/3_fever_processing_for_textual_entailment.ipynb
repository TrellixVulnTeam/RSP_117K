{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook represents an alternative to generating labelled data for textual entailment.\n",
    "Differently from `0_fever_decode_wikipage_IDs_to_sentences`, this one takes the FEVER data directly from the processed data from the kgat team.\n",
    "\n",
    "This is better for one main reason: where the other notebook created *a single completely random sentence* for the NOT ENOUGH INFO cases, this one is able to draw from pre-selected sentences that, *while related to the claim, do not support nor refute it*.\n",
    "\n",
    "This is very important, as the former approach meant the data was very easy to perform well in, as the sentences were simply completely unrelated, and much clearer to define as \"not argumentative\" to the claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import sqlite3 as sql\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "HOME = Path('/home/k20036346/')\n",
    "KERNELGAT_DATA = HOME / 'Repos/KernelGAT/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, Value\n",
    "\n",
    "features = Features(\n",
    "{\n",
    "    'id': Value('int64'),\n",
    "    'label': Value('string'),\n",
    "    'claim': Value('string'),\n",
    "    'evidence': Value('string'),\n",
    "    'is_multihop' : Value('bool'), \n",
    "})\n",
    "\n",
    "PARTITIONS = ['train','dev','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11790/4066666287.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'evidence'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evidence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evidence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     dataset_dict[partition] = Dataset.from_pandas(\n",
      "\u001b[0;32m~/.conda/envs/cuda/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5813\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5814\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5815\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5816\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cuda/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     def convert(\n",
      "\u001b[0;32m~/.conda/envs/cuda/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cuda/lib/python3.9/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cuda/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cuda/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cuda/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_string_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_na_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_datetime64_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cuda/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.ensure_string_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cuda/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.ensure_string_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pandas import read_json\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset_dict = {}\n",
    "for partition in PARTITIONS:\n",
    "    df = read_json(KERNELGAT_DATA / f'bert_{partition}.json', lines=True)\n",
    "    \n",
    "    if 'evidence' in df.columns:\n",
    "        df['evidence'] = df['evidence'].astype(str)\n",
    "              \n",
    "    dataset_dict[partition] = Dataset.from_pandas(\n",
    "        df,\n",
    "        features=Features({column: features[column] for column in df.columns})\n",
    "    )\n",
    "    \n",
    "data_bert = DatasetDict(dataset_dict)\n",
    "data_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "rep_double_whitespace = lambda x : _RE_COMBINE_WHITESPACE.sub(\" \", x).strip()\n",
    "\n",
    "for partition in PARTITIONS:\n",
    "    if partition == 'test':\n",
    "        continue\n",
    "    gd_all_df = []\n",
    "    are_multihop = []\n",
    "    with open(KERNELGAT_DATA / f'all_{partition}.json','r') as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            row['golden_evidence'] = row.pop('evidence')\n",
    "            row['golden_id'] = row.pop('id')\n",
    "            row['golden_claim'] = row.pop('claim')\n",
    "            row['golden_label'] = row.pop('label')\n",
    "            row['is_multihop'] = (len([e for e in row['golden_evidence'] if e[3] == 2])>0)\n",
    "            gd_all_df.append(row)\n",
    "\n",
    "    gd_all_df = pd.DataFrame(gd_all_df)\n",
    "\n",
    "    assert data_bert[partition]['id'] == gd_all_df['golden_id'].tolist()\n",
    "    assert data_bert[partition]['claim'] == gd_all_df['golden_claim'].apply(rep_double_whitespace).tolist()\n",
    "    assert data_bert[partition]['label'] == gd_all_df['golden_label'].tolist()\n",
    "\n",
    "    data_bert[partition] = data_bert[partition].add_column('golden_evidence', gd_all_df['golden_evidence'].astype(str))\n",
    "    data_bert[partition] = data_bert[partition].add_column('is_multihop', gd_all_df['is_multihop'].tolist())\n",
    "\n",
    "data_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in PARTITIONS:\n",
    "    print(partition.upper(), '\\n')\n",
    "    \n",
    "    if partition != 'test':\n",
    "        print('Labels:')\n",
    "        print(pd.Series(data_bert[partition]['label']).value_counts())\n",
    "    print(f'Total: {len(data_bert[partition])}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFTER REMOVING MULTIHOPS\n",
    "for partition in PARTITIONS:\n",
    "    \n",
    "    print(partition.upper(), '\\n')\n",
    "    original_size = len(data_bert[partition])\n",
    "    if partition != 'test':\n",
    "        data_bert[partition] = data_bert[partition].filter(lambda x : not x['is_multihop'])\n",
    "        print('Labels:')\n",
    "        print(pd.Series(data_bert[partition]['label']).value_counts())\n",
    "        data_bert[partition] = data_bert[partition].remove_columns(['is_multihop'])\n",
    "        \n",
    "    print(f'Total: {len(data_bert[partition])}, loss of {round(100 - 100*len(data_bert[partition])/original_size,3)}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FROM THIS POINT ON WE WORK WITH DATAFRAMES DIRECTLY INSTEAD OF DATASET CLASS\n",
    "MUCH EASIER\n",
    "'''\n",
    "\n",
    "data_bert_df = {\n",
    "    partition: data_bert[partition].to_pandas() for partition in PARTITIONS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "To create the support detection dataset, the first strategy will be:\n",
    "    For each SUPPORTS claim-evidence pair, we keep the pair if the evidence_score is 1 (is gold truth);\n",
    "    For each REJECTS claim-evidence pair, we do the same as above;\n",
    "    For each NOT ENOUGH INFO claim-evidence pair, we select all evidence pairs that received a\n",
    "        score over 0 from BERT sentence retrieval\n",
    "'''\n",
    "\n",
    "from ast import literal_eval as leval\n",
    "from unicodedata import normalize as uninorm\n",
    "\n",
    "def filter_for_support_detection(row): \n",
    "    \n",
    "    for ev in ['evidence', 'golden_evidence']:\n",
    "        if ev not in row:\n",
    "            continue\n",
    "        row[ev] = leval(row[ev])\n",
    "        row[ev] = [\n",
    "            [\n",
    "                uninorm('NFKC', v[0]),\n",
    "                v[1],\n",
    "                uninorm('NFKC', v[2]),\n",
    "                v[3]\n",
    "            ] for v in row[ev]\n",
    "        ]\n",
    "        \n",
    "    if 'label' not in row:        \n",
    "        return(row)\n",
    "    \n",
    "    if row['label'] == 'NOT ENOUGH INFO':\n",
    "        row_evidence = []\n",
    "        for evidence in row['evidence']:\n",
    "            keep = False\n",
    "            if evidence[3] > 0.0:\n",
    "                keep = True\n",
    "            else:\n",
    "                for gd_evidence in row['golden_evidence']:\n",
    "                    if ((evidence[0],evidence[1]) == (gd_evidence[0],gd_evidence[1])) and gd_evidence[3] == 1:\n",
    "                        print(row)\n",
    "                        # This is to cover the case where bert_train.json gave score <=0 to an evidence\n",
    "                        # that is actually golden data. Since bert_train.json is built on all_train.json,\n",
    "                        # this should not happen\n",
    "                        keep = True\n",
    "            if keep:\n",
    "                row_evidence.append(evidence)\n",
    "        row['evidence'] = row_evidence\n",
    "    else:\n",
    "        row['evidence'] = [e for e in row['evidence'] if e[3] == 1]\n",
    "        # KEEP ALL EVIDENCE WITH GD=1 (TRUE EVIDENCE)\n",
    "        # Both evidence list from all_train.json and bert_train.json have e[3] == 1 for golden data\n",
    "    \n",
    "    return(row)\n",
    "\n",
    "data_bert_df = {k: v.apply(filter_for_support_detection, axis=1) for k,v in data_bert_df.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we drop the golden_evidence column, it served its purpose\n",
    "# We also explode the evidence column so each evidence has its own row\n",
    "\n",
    "data_bert_df = {\n",
    "    k: v.drop('golden_evidence', axis=1, errors='ignore').explode('evidence').reset_index(drop=True)\n",
    "    for k,v in data_bert_df.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some unverifiable claims do not have evidence recovered by the document retrieval\n",
    "\n",
    "data_bert_df = {\n",
    "    k: v[v['evidence'].apply(lambda x: type(x) == list)].reset_index(drop=True)\n",
    "    for k,v in data_bert_df.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding the evidence row\n",
    "\n",
    "for k,v in data_bert_df.items():\n",
    "    data_bert_df[k][['evidence_page','evidence_line','evidence_text','evidence_score']] =\\\n",
    "        pd.DataFrame(data_bert_df[k]['evidence'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sent(sentence):\n",
    "    sentence = re.sub(\"LSB.*?RSB\", \"\", sentence)\n",
    "    sentence = re.sub(\"LRB RRB \", \"\", sentence)\n",
    "    sentence = re.sub(\"LRB\", \" ( \", sentence)\n",
    "    sentence = re.sub(\"RRB\", \" )\", sentence)\n",
    "    sentence = re.sub(\"--\", \"-\", sentence)\n",
    "    sentence = re.sub(\"``\", '\"', sentence)\n",
    "    sentence = re.sub(\"''\", '\"', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def process_wiki_title(title):\n",
    "    title = re.sub(\"_\", \" \", title)\n",
    "    title = re.sub(\" -LRB-\", \" ( \", title)\n",
    "    title = re.sub(\"-RRB-\", \" )\", title)\n",
    "    title = re.sub(\"-COLON-\", \":\", title)\n",
    "    return title\n",
    "\n",
    "#for k,v in data_bert_df.items():\n",
    "#    data_bert_df[k]['evidence_page'] =\\\n",
    "#        data_bert_df[k]['evidence_page'].apply(process_wiki_title)\n",
    "#    \n",
    "#for k,v in data_bert_df.items():\n",
    "#    data_bert_df[k]['evidence_text'] =\\\n",
    "#        data_bert_df[k]['evidence_text'].apply(process_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "SUPPORTS           81583\n",
      "NOT ENOUGH INFO    59114\n",
      "REFUTES            33263\n",
      "Name: label, dtype: int64\n",
      "\n",
      "DEV\n",
      "NOT ENOUGH INFO    11117\n",
      "REFUTES             7438\n",
      "SUPPORTS            6839\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('TRAIN')\n",
    "print(data_bert_df['train'].label.value_counts())\n",
    "print()\n",
    "\n",
    "print('DEV')\n",
    "print(data_bert_df['dev'].label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in PARTITIONS:\n",
    "    data_bert_df[partition].to_csv(f'./data/support_data_v2/{partition}_support_from_bert_SPECIAL_CHARS_CODED.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
