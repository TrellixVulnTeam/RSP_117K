{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "335aaebd",
   "metadata": {},
   "source": [
    "This notebook uses the data produced by `0_fever_decode_wikipage_IDs_to_sentences` to train and evaluate a BERT model for textual entailment.\n",
    "\n",
    "It uses the claim, the labels, and the *first sentence only* of the datasets generated in the mentioned notebook.\n",
    "\n",
    "This is a **bad approach**, and we latter choose to follow a much better one.\n",
    "However, I still need to document this in order to understand what I did 1y ago and why.\n",
    "\n",
    "This version trains for 2 classes only (Supports, Refutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hindu-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "from seaborn import displot\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "decreased-trace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_NAME = 'bert-base-uncased' # We could use bert-large with the new GPU from gravity. Would it perform better?\n",
    "# It probably would be better to check after the whole pipeline is tested. Also it probably should not be uncased, so let's\n",
    "# test that too!\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 2 # On gravity, we were told to use 2 workers. Perhaps because we have 2 GPUs?\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recovered-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_csv('./data/train_wikidecoded.csv', encoding='UTF-8')\n",
    "df_dev = read_csv('./data/shared_task_dev_wikidecoded.csv', encoding='UTF-8')\n",
    "df_test = read_csv('./data/shared_task_test_wikidecoded.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "strategic-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train.verifiable.apply(lambda x : x=='VERIFIABLE')].reset_index(drop=True)\n",
    "df_dev = df_dev[df_dev.verifiable.apply(lambda x : x=='VERIFIABLE')].reset_index(drop=True)\n",
    "# Yeah, this is Support/Refute only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "professional-shareware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((109810, 3), (6666, 3), (6666, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_keep = ['claim','first_sentence','label_numeric']\n",
    "# And we also only use the first sentence. This is problematic, as it greatly reduces the variability\n",
    "# of evidence-claim pairings the model could learn.\n",
    "\n",
    "df_train = df_train[columns_to_keep]\n",
    "df_dev = df_dev[columns_to_keep]\n",
    "\n",
    "df_test, df_dev = train_test_split(df_dev, test_size=.5)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_dev = df_dev.reset_index(drop=True)\n",
    "df_train.shape, df_test.shape, df_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "difficult-sacramento",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    80035\n",
       "1    29775\n",
       "Name: label_numeric, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label_numeric.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unusual-samoa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0f39fd3dad462489db97b5d8ce0ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d310b22b5914713b189b327cc14968e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc78eab12655488ebf3d0c465bc4d55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663774f0e7874135b6291a4c3908a339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, fast=True)\n",
    "# standard Bert tokenizer. don't know why we make it fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "criminal-convergence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_lengths = False\n",
    "\n",
    "if check_lengths:\n",
    "    # This helps see what a good max length would be\n",
    "    def get_tokenized_len(row):\n",
    "        tokens = tokenizer.encode(row['claim'], row['first_sentence'], max_length=512, truncation='only_second')\n",
    "        return len(tokens)\n",
    "\n",
    "    for df in [df_train, df_test, df_dev]:\n",
    "        df['tokenized_len'] = df.apply(get_tokenized_len, axis=1)\n",
    "\n",
    "    tokenized_lens = df_train['tokenized_len'].tolist() +\\\n",
    "        df_test['tokenized_len'].tolist() +\\\n",
    "        df_dev['tokenized_len'].tolist()\n",
    "\n",
    "    displot(tokenized_lens)\n",
    "    plt.xlim([0, 512]);\n",
    "    plt.xlabel('Token count');\n",
    "\n",
    "    print(np.max(tokenized_lens))\n",
    "\n",
    "MAX_LEN = 350 #This means a few cases get cut down. I don't think they're many, but also don't think there\n",
    "# is a big drop in performance if we let them be? I would compare performance with MAX_LEN=425 and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "circular-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So here we reate dataset and dataloader instances for the FEVER data from the bad approach\n",
    "# torch.utils.data.Dataset and torch.utils.data.DataLoader were used\n",
    "# information about them can be found here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "class FEVERDataset(Dataset):\n",
    "    def __init__(self, claims, sentences, labels, tokenizer, max_len):\n",
    "        self.claims=claims\n",
    "        self.sentences=sentences\n",
    "        self.labels=labels\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_len=max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.claims))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        claim = self.claims[idx]\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            claim,\n",
    "            sentence,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        return {\n",
    "            'claim': claim,\n",
    "            'sentence': sentence,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long).to(DEVICE)\n",
    "        }\n",
    "    \n",
    "def to_data_loader(df, tokenizer, max_len, batch_size, num_workers):\n",
    "    dataset = FEVERDataset(\n",
    "        claims = df.claim,\n",
    "        sentences = df.first_sentence,\n",
    "        labels = df.label_numeric,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return dataset, DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "train_dataset, train_dataloader = to_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE, NUM_WORKERS)\n",
    "test_dataset, test_dataloader = to_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE, NUM_WORKERS)\n",
    "dev_dataset, dev_dataloader = to_data_loader(df_dev, tokenizer, MAX_LEN, BATCH_SIZE, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "critical-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92bc59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b7acf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  9 19:29:00 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 3070    On   | 00000000:5E:00.0 Off |                  N/A |\r\n",
      "| 30%   28C    P8    17W / 220W |   1926MiB /  7982MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 3070    On   | 00000000:AF:00.0 Off |                  N/A |\r\n",
      "| 30%   27C    P8    20W / 220W |   1420MiB /  7982MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A    139771      C   ...onda/envs/cuda/bin/python     1923MiB |\r\n",
      "|    1   N/A  N/A    139771      C   ...onda/envs/cuda/bin/python     1417MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "organized-phenomenon",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/k20036346/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/k20036346/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 109810\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 5148\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5148' max='5148' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5148/5148 2:08:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.291674</td>\n",
       "      <td>0.894689</td>\n",
       "      <td>0.895815</td>\n",
       "      <td>0.876561</td>\n",
       "      <td>0.915933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.275261</td>\n",
       "      <td>0.897840</td>\n",
       "      <td>0.890602</td>\n",
       "      <td>0.946075</td>\n",
       "      <td>0.841275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.281051</td>\n",
       "      <td>0.904740</td>\n",
       "      <td>0.897762</td>\n",
       "      <td>0.956104</td>\n",
       "      <td>0.846131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>0.298520</td>\n",
       "      <td>0.909241</td>\n",
       "      <td>0.903339</td>\n",
       "      <td>0.953779</td>\n",
       "      <td>0.857967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.308633</td>\n",
       "      <td>0.906241</td>\n",
       "      <td>0.899952</td>\n",
       "      <td>0.952236</td>\n",
       "      <td>0.853111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.134700</td>\n",
       "      <td>0.283806</td>\n",
       "      <td>0.913741</td>\n",
       "      <td>0.908541</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.866768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.288670</td>\n",
       "      <td>0.915692</td>\n",
       "      <td>0.911216</td>\n",
       "      <td>0.950247</td>\n",
       "      <td>0.875266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.311922</td>\n",
       "      <td>0.914191</td>\n",
       "      <td>0.909004</td>\n",
       "      <td>0.955199</td>\n",
       "      <td>0.867071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.087600</td>\n",
       "      <td>0.293087</td>\n",
       "      <td>0.919292</td>\n",
       "      <td>0.915169</td>\n",
       "      <td>0.952412</td>\n",
       "      <td>0.880728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.303332</td>\n",
       "      <td>0.919292</td>\n",
       "      <td>0.914820</td>\n",
       "      <td>0.956306</td>\n",
       "      <td>0.876783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-500/pytorch_model.bin\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-1000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-1000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-1000/pytorch_model.bin\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-1500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-1500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-2000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-2000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-1000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-2500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-2500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-1500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-3000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-3000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-2000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-3500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-3500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-2500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-4000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-4000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-3000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-4500\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-4500/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-3500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-5000\n",
      "Configuration saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-5000/config.json\n",
      "Model weights saved in ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-4000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading best model from ./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/checkpoint-4500 (score: 0.9151687164932198).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5148, training_loss=0.15975915357505246, metrics={'train_runtime': 7725.6165, 'train_samples_per_second': 42.641, 'train_steps_per_second': 0.666, 'total_flos': 5.925163327839e+16, 'train_loss': 0.15975915357505246, 'epoch': 3.0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./run/1_Using_BERT_on_FEVER_with_Trainer_2_Classes/',\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",     # Evaluation is done at the end of each epoch.\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints. \n",
    "    dataloader_pin_memory=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics      # metrics to be computed\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "immediate-developer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='964' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 33:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2930874824523926,\n",
       " 'eval_accuracy': 0.9192919291929192,\n",
       " 'eval_f1': 0.9151687164932198,\n",
       " 'eval_precision': 0.9524122087298983,\n",
       " 'eval_recall': 0.8807283763277693,\n",
       " 'eval_runtime': 33.4941,\n",
       " 'eval_samples_per_second': 199.02,\n",
       " 'eval_steps_per_second': 1.582,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25021100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6666\n",
      "  Batch size = 128\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2848038673400879,\n",
       " 'eval_accuracy': 0.9176417641764176,\n",
       " 'eval_f1': 0.9152385363594256,\n",
       " 'eval_precision': 0.9542820347714102,\n",
       " 'eval_recall': 0.8792643132601602,\n",
       " 'eval_runtime': 33.6035,\n",
       " 'eval_samples_per_second': 198.372,\n",
       " 'eval_steps_per_second': 1.577,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d949d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 109810\n",
      "  Batch size = 128\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.06054077297449112,\n",
       " 'eval_accuracy': 0.982132774792824,\n",
       " 'eval_f1': 0.9671576832942752,\n",
       " 'eval_precision': 0.9640914400133489,\n",
       " 'eval_recall': 0.9702434928631403,\n",
       " 'eval_runtime': 562.4108,\n",
       " 'eval_samples_per_second': 195.249,\n",
       " 'eval_steps_per_second': 1.526,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
