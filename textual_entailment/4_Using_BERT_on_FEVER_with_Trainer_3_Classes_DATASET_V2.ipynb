{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the data produced by `3_fever_processing_for_textual_entailment` to train and evaluate a BERT model for textual entailment.\n",
    "\n",
    "The data there is intended to be a direct improvement over the one used before (v1). It does not only use the *first sentence* of an evidence set, but all sentences from the evidence set which can be enough evidence *alone* (so, excluding multihop reasoning cases).\n",
    "\n",
    "We also fill NOT ENOUGH INFO cases with random sentences *from the pre-selected ones by the retrieval system*. This means they are still related to the claim, just no entailment can be concluded.\n",
    "\n",
    "This version trains for 3 classes (Supports, Refutes, and Not Enough Info).\n",
    "\n",
    "This BERT model will later be put through hyperparameter search so we can try to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "from seaborn import displot, boxplot\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_NAME = 'bert-base-uncased' # We could use bert-large with the new GPU from gravity. Would it perform better?\n",
    "# It probably would be better to check after the whole pipeline is tested. Also it probably should not be uncased, so let's\n",
    "# test that too!\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 2 # On gravity, we were told to use 2 workers. Perhaps because we have 2 GPUs?\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'evidence', 'claim', 'label', 'evidence_page', 'evidence_line',\n",
       "       'evidence_text', 'evidence_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = read_csv('./data/support_data_v2/train_support_from_bert_SPECIAL_CHARS_CODED.csv', encoding='UTF-8')\n",
    "df_dev = read_csv('./data/support_data_v2/dev_support_from_bert_SPECIAL_CHARS_CODED.csv', encoding='UTF-8')\n",
    "df_test = read_csv('./data/support_data_v2/test_support_from_bert_SPECIAL_CHARS_CODED.csv', encoding='UTF-8')\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['SUPPORTS', 'REFUTES', 'NOT ENOUGH INFO']\n",
    "def label_to_numeric(row):\n",
    "    if 'label' in row:\n",
    "        return LABELS.index(row['label'])\n",
    "    return -1\n",
    "\n",
    "df_train['label_numeric'] = df_train.apply(label_to_numeric, axis=1)\n",
    "df_dev['label_numeric'] = df_dev.apply(label_to_numeric, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((173960, 3), (12697, 3), (12697, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_keep = ['claim','evidence_text','label_numeric']\n",
    "df_train = df_train[columns_to_keep]\n",
    "df_dev = df_dev[columns_to_keep]\n",
    "\n",
    "df_test, df_dev = train_test_split(df_dev, test_size=.5)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_dev = df_dev.reset_index(drop=True)\n",
    "df_train.shape, df_test.shape, df_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: TAKING UNRELATED TEXT (LOW RETRIEVAL SCORE) TO THE CLAIMS AS 'NOT ENOUGH INFO' MIGHT ALSO BE A GOOD IDEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         He also has another YouTube channel called `` ...\n",
       "1         He is best known for his vlogs , where he post...\n",
       "2         The subsequent expansion of the list of princi...\n",
       "3         Adrienne Eliza Houghton LRB née Bailon ; born ...\n",
       "4         The group briefly disbanded in August 2006 and...\n",
       "                                ...                        \n",
       "173955    A dwarf warrior , he is the son of Glóin LRB a...\n",
       "173956    Gimli is a fictional character from J. R. R. T...\n",
       "173957    She is an Academy Award and BAFTA Award winner...\n",
       "173958    She has also won the BAFTA Award for Best Actr...\n",
       "173959    She was appointed a UNICEF Goodwill Ambassador...\n",
       "Name: evidence_text, Length: 173960, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['evidence_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    81583\n",
       "2    59114\n",
       "1    33263\n",
       "Name: label_numeric, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label_numeric.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, fast=True)\n",
    "# standard Bert tokenizer. don't know why we make it fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_lengths = True\n",
    "\n",
    "if check_lengths:\n",
    "    # This helps see what a good max length would be\n",
    "    def get_tokenized_len(row):\n",
    "        tokens = tokenizer.encode(row['claim'], row['evidence_text'], max_length=512, truncation=True)\n",
    "        return len(tokens)\n",
    "\n",
    "    for df in [df_train, df_test, df_dev]:\n",
    "        df['tokenized_len'] = df.apply(get_tokenized_len, axis=1)\n",
    "\n",
    "    tokenized_lens = df_train['tokenized_len'].tolist() +\\\n",
    "        df_test['tokenized_len'].tolist() +\\\n",
    "        df_dev['tokenized_len'].tolist()\n",
    "\n",
    "    boxplot(x=tokenized_lens)\n",
    "    plt.xlim([0, 512]);\n",
    "    plt.xlabel('Token count');\n",
    "\n",
    "    #print(np.max(tokenized_lens))\n",
    "    \n",
    "MAX_LEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So here we reate dataset and dataloader instances for the FEVER data from the bad approach\n",
    "# torch.utils.data.Dataset and torch.utils.data.DataLoader were used\n",
    "# information about them can be found here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "class FEVERDataset(Dataset):\n",
    "    def __init__(self, claims, sentences, labels, tokenizer, max_len):\n",
    "        self.claims=claims\n",
    "        self.sentences=sentences\n",
    "        self.labels=labels\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_len=max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.claims))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        claim = self.claims[idx]\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            claim,\n",
    "            sentence,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        return {\n",
    "            'claim': claim,\n",
    "            'sentence': sentence,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long).to(DEVICE)\n",
    "        }\n",
    "    \n",
    "def to_data_loader(df, tokenizer, max_len, batch_size, num_workers):\n",
    "    dataset = FEVERDataset(\n",
    "        claims = df.claim,\n",
    "        sentences = df.evidence_text,\n",
    "        labels = df.label_numeric,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return dataset, DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "train_dataset, train_dataloader = to_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE, NUM_WORKERS)\n",
    "test_dataset, test_dataloader = to_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE, NUM_WORKERS)\n",
    "dev_dataset, dev_dataloader = to_data_loader(df_dev, tokenizer, MAX_LEN, BATCH_SIZE, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 17 19:48:43 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 3070    On   | 00000000:5E:00.0 Off |                  N/A |\r\n",
      "| 30%   28C    P8    18W / 220W |      3MiB /  7982MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 3070    On   | 00000000:AF:00.0 Off |                  N/A |\r\n",
      "| 30%   27C    P8    19W / 220W |      3MiB /  7982MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 173960\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32619\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29501' max='32619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29501/32619 3:45:53 < 23:52, 2.18 it/s, Epoch 2.71/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.863200</td>\n",
       "      <td>0.820782</td>\n",
       "      <td>0.645586</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>0.653496</td>\n",
       "      <td>0.665978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.631600</td>\n",
       "      <td>0.778402</td>\n",
       "      <td>0.681421</td>\n",
       "      <td>0.680587</td>\n",
       "      <td>0.696011</td>\n",
       "      <td>0.697408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.764455</td>\n",
       "      <td>0.708435</td>\n",
       "      <td>0.707997</td>\n",
       "      <td>0.715485</td>\n",
       "      <td>0.721249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.569500</td>\n",
       "      <td>0.703443</td>\n",
       "      <td>0.710325</td>\n",
       "      <td>0.711487</td>\n",
       "      <td>0.716274</td>\n",
       "      <td>0.727433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.569700</td>\n",
       "      <td>0.705894</td>\n",
       "      <td>0.722218</td>\n",
       "      <td>0.722585</td>\n",
       "      <td>0.726266</td>\n",
       "      <td>0.734584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.532200</td>\n",
       "      <td>0.704483</td>\n",
       "      <td>0.723714</td>\n",
       "      <td>0.723123</td>\n",
       "      <td>0.729020</td>\n",
       "      <td>0.735335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.533900</td>\n",
       "      <td>0.700152</td>\n",
       "      <td>0.716705</td>\n",
       "      <td>0.716034</td>\n",
       "      <td>0.727284</td>\n",
       "      <td>0.730520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.723216</td>\n",
       "      <td>0.729149</td>\n",
       "      <td>0.730790</td>\n",
       "      <td>0.731653</td>\n",
       "      <td>0.745254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.531100</td>\n",
       "      <td>0.771646</td>\n",
       "      <td>0.708750</td>\n",
       "      <td>0.704927</td>\n",
       "      <td>0.722774</td>\n",
       "      <td>0.718364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.677016</td>\n",
       "      <td>0.740647</td>\n",
       "      <td>0.740079</td>\n",
       "      <td>0.750020</td>\n",
       "      <td>0.743957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>0.652328</td>\n",
       "      <td>0.745530</td>\n",
       "      <td>0.744916</td>\n",
       "      <td>0.752537</td>\n",
       "      <td>0.747503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.506900</td>\n",
       "      <td>0.660799</td>\n",
       "      <td>0.743404</td>\n",
       "      <td>0.741178</td>\n",
       "      <td>0.755268</td>\n",
       "      <td>0.741368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.493800</td>\n",
       "      <td>0.644833</td>\n",
       "      <td>0.744113</td>\n",
       "      <td>0.746767</td>\n",
       "      <td>0.742650</td>\n",
       "      <td>0.762234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.476700</td>\n",
       "      <td>0.736599</td>\n",
       "      <td>0.716390</td>\n",
       "      <td>0.719164</td>\n",
       "      <td>0.721454</td>\n",
       "      <td>0.744472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.666617</td>\n",
       "      <td>0.737812</td>\n",
       "      <td>0.738178</td>\n",
       "      <td>0.745115</td>\n",
       "      <td>0.746602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.481800</td>\n",
       "      <td>0.720497</td>\n",
       "      <td>0.741277</td>\n",
       "      <td>0.743001</td>\n",
       "      <td>0.742018</td>\n",
       "      <td>0.757227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.457200</td>\n",
       "      <td>0.628078</td>\n",
       "      <td>0.751831</td>\n",
       "      <td>0.753966</td>\n",
       "      <td>0.750712</td>\n",
       "      <td>0.763028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.457800</td>\n",
       "      <td>0.689641</td>\n",
       "      <td>0.749862</td>\n",
       "      <td>0.751430</td>\n",
       "      <td>0.751427</td>\n",
       "      <td>0.760106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.457700</td>\n",
       "      <td>0.641200</td>\n",
       "      <td>0.752934</td>\n",
       "      <td>0.753519</td>\n",
       "      <td>0.753664</td>\n",
       "      <td>0.760240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.720339</td>\n",
       "      <td>0.728440</td>\n",
       "      <td>0.729451</td>\n",
       "      <td>0.735056</td>\n",
       "      <td>0.743712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.447200</td>\n",
       "      <td>0.703375</td>\n",
       "      <td>0.731748</td>\n",
       "      <td>0.733844</td>\n",
       "      <td>0.735943</td>\n",
       "      <td>0.750282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.756354</td>\n",
       "      <td>0.738915</td>\n",
       "      <td>0.741368</td>\n",
       "      <td>0.738430</td>\n",
       "      <td>0.756747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.356600</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.733323</td>\n",
       "      <td>0.736666</td>\n",
       "      <td>0.732933</td>\n",
       "      <td>0.755956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.355400</td>\n",
       "      <td>0.738775</td>\n",
       "      <td>0.739545</td>\n",
       "      <td>0.742578</td>\n",
       "      <td>0.738583</td>\n",
       "      <td>0.757858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.782406</td>\n",
       "      <td>0.731433</td>\n",
       "      <td>0.734143</td>\n",
       "      <td>0.732289</td>\n",
       "      <td>0.752016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.354700</td>\n",
       "      <td>0.766923</td>\n",
       "      <td>0.739702</td>\n",
       "      <td>0.742514</td>\n",
       "      <td>0.738884</td>\n",
       "      <td>0.758108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.359200</td>\n",
       "      <td>0.721261</td>\n",
       "      <td>0.747578</td>\n",
       "      <td>0.748463</td>\n",
       "      <td>0.750090</td>\n",
       "      <td>0.757588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.353700</td>\n",
       "      <td>0.766043</td>\n",
       "      <td>0.730645</td>\n",
       "      <td>0.733567</td>\n",
       "      <td>0.732652</td>\n",
       "      <td>0.750895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>0.793536</td>\n",
       "      <td>0.737340</td>\n",
       "      <td>0.740434</td>\n",
       "      <td>0.737294</td>\n",
       "      <td>0.755928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>0.890772</td>\n",
       "      <td>0.730488</td>\n",
       "      <td>0.733781</td>\n",
       "      <td>0.731213</td>\n",
       "      <td>0.752062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.734023</td>\n",
       "      <td>0.736158</td>\n",
       "      <td>0.738328</td>\n",
       "      <td>0.737765</td>\n",
       "      <td>0.752774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.704950</td>\n",
       "      <td>0.742931</td>\n",
       "      <td>0.745026</td>\n",
       "      <td>0.744382</td>\n",
       "      <td>0.757726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.736818</td>\n",
       "      <td>0.750965</td>\n",
       "      <td>0.753034</td>\n",
       "      <td>0.750942</td>\n",
       "      <td>0.763592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.337400</td>\n",
       "      <td>0.771972</td>\n",
       "      <td>0.748208</td>\n",
       "      <td>0.750773</td>\n",
       "      <td>0.748607</td>\n",
       "      <td>0.760891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.335400</td>\n",
       "      <td>0.774941</td>\n",
       "      <td>0.739151</td>\n",
       "      <td>0.741946</td>\n",
       "      <td>0.741401</td>\n",
       "      <td>0.756754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.717737</td>\n",
       "      <td>0.744743</td>\n",
       "      <td>0.747562</td>\n",
       "      <td>0.744321</td>\n",
       "      <td>0.760869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.316400</td>\n",
       "      <td>0.739767</td>\n",
       "      <td>0.755060</td>\n",
       "      <td>0.758347</td>\n",
       "      <td>0.752963</td>\n",
       "      <td>0.771292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.763305</td>\n",
       "      <td>0.738915</td>\n",
       "      <td>0.740778</td>\n",
       "      <td>0.744016</td>\n",
       "      <td>0.754922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.753132</td>\n",
       "      <td>0.750965</td>\n",
       "      <td>0.752829</td>\n",
       "      <td>0.752356</td>\n",
       "      <td>0.762164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.311400</td>\n",
       "      <td>0.811653</td>\n",
       "      <td>0.744113</td>\n",
       "      <td>0.747305</td>\n",
       "      <td>0.744225</td>\n",
       "      <td>0.765577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.766379</td>\n",
       "      <td>0.744979</td>\n",
       "      <td>0.747911</td>\n",
       "      <td>0.744783</td>\n",
       "      <td>0.763964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.832151</td>\n",
       "      <td>0.740647</td>\n",
       "      <td>0.743873</td>\n",
       "      <td>0.740157</td>\n",
       "      <td>0.760028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.812859</td>\n",
       "      <td>0.743483</td>\n",
       "      <td>0.745942</td>\n",
       "      <td>0.745253</td>\n",
       "      <td>0.759282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.255900</td>\n",
       "      <td>1.011203</td>\n",
       "      <td>0.739466</td>\n",
       "      <td>0.742826</td>\n",
       "      <td>0.739138</td>\n",
       "      <td>0.760480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>1.045794</td>\n",
       "      <td>0.737576</td>\n",
       "      <td>0.740597</td>\n",
       "      <td>0.737619</td>\n",
       "      <td>0.757753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>0.744585</td>\n",
       "      <td>0.748061</td>\n",
       "      <td>0.742947</td>\n",
       "      <td>0.762497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.221500</td>\n",
       "      <td>1.075843</td>\n",
       "      <td>0.741041</td>\n",
       "      <td>0.743633</td>\n",
       "      <td>0.741350</td>\n",
       "      <td>0.759100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.973703</td>\n",
       "      <td>0.735764</td>\n",
       "      <td>0.738946</td>\n",
       "      <td>0.736103</td>\n",
       "      <td>0.757523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.983476</td>\n",
       "      <td>0.749153</td>\n",
       "      <td>0.751879</td>\n",
       "      <td>0.749709</td>\n",
       "      <td>0.763542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>0.904971</td>\n",
       "      <td>0.738285</td>\n",
       "      <td>0.741688</td>\n",
       "      <td>0.737299</td>\n",
       "      <td>0.759069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.974084</td>\n",
       "      <td>0.748051</td>\n",
       "      <td>0.751376</td>\n",
       "      <td>0.746405</td>\n",
       "      <td>0.764878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.224400</td>\n",
       "      <td>1.032817</td>\n",
       "      <td>0.742695</td>\n",
       "      <td>0.745717</td>\n",
       "      <td>0.742246</td>\n",
       "      <td>0.759221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.217600</td>\n",
       "      <td>1.001031</td>\n",
       "      <td>0.738993</td>\n",
       "      <td>0.742109</td>\n",
       "      <td>0.738908</td>\n",
       "      <td>0.758969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.209900</td>\n",
       "      <td>1.044459</td>\n",
       "      <td>0.744113</td>\n",
       "      <td>0.746848</td>\n",
       "      <td>0.744129</td>\n",
       "      <td>0.759185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.214700</td>\n",
       "      <td>1.031260</td>\n",
       "      <td>0.746712</td>\n",
       "      <td>0.749491</td>\n",
       "      <td>0.746312</td>\n",
       "      <td>0.762895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.219000</td>\n",
       "      <td>0.986118</td>\n",
       "      <td>0.747263</td>\n",
       "      <td>0.750336</td>\n",
       "      <td>0.746214</td>\n",
       "      <td>0.764526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>1.020953</td>\n",
       "      <td>0.739151</td>\n",
       "      <td>0.742484</td>\n",
       "      <td>0.739044</td>\n",
       "      <td>0.761498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.015970</td>\n",
       "      <td>0.742301</td>\n",
       "      <td>0.745325</td>\n",
       "      <td>0.741808</td>\n",
       "      <td>0.762053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 14/100 00:06 < 00:45, 1.87 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-17000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-1000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-1000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-32500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-1500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-1500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-2000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-2000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-1000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-2500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-2500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-1500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-3000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-3000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-2000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-3500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-3500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-2500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-4000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-4000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-3000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-4500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-4500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-3500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-5000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-5000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-4000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-5500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-5500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-4500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-6000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-6000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-5000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-6500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-6500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-5500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-7000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-7000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-6000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-7500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-7500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-7000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-8000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-8000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-7500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-8500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-8500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-6500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-9000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-9000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-8000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-9500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-9500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-9000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-10000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-10000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-9500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-10500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-10500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-10000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-11000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-11000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-10500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-11500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-11500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-11000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-12000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-12000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-11500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-12500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-12500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-12000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-13000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-13000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-12500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-13500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-13500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-13000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-14000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-14000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-13500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-14500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-14500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-14000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-15000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-15000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-14500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-15500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-15500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-15000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-16000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-16000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-15500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-16500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-16500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-16000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-17000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-17000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-16500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-17500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-17500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-17000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-18000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-18000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-17500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-18500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-18500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-8500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-19000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-19000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-18000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-19500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-19500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-19000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-20000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-20000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-19500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-20500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-20500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-20000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-21000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-21000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-20500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-27000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-27000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-26500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-27500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-27500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-27000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-28000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-28000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-27500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-28500\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-28500/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-28000] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-29000\n",
      "Configuration saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-29000/config.json\n",
      "Model weights saved in ./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/checkpoint-28500] due to args.save_total_limit\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./run/4_Using_BERT_on_FEVER_with_Trainer_3_Classes_DATASET_V2/',\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",     # Evaluation is done at the end of each epoch.\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints. \n",
    "    dataloader_pin_memory=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels = 3).to(DEVICE)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics      # metrics to be computed\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1560' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 14:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.73976731300354,\n",
       " 'eval_accuracy': 0.7550602504528628,\n",
       " 'eval_f1': 0.7583467931579047,\n",
       " 'eval_precision': 0.752962919489128,\n",
       " 'eval_recall': 0.7712917304505954,\n",
       " 'eval_runtime': 53.315,\n",
       " 'eval_samples_per_second': 238.151,\n",
       " 'eval_steps_per_second': 1.876,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "\n",
    "#{'eval_loss': 0.7109167575836182,\n",
    "# 'eval_accuracy': 0.7558478380719855,\n",
    "# 'eval_f1': 0.7575068958603147,\n",
    "# 'eval_precision': 0.7545204429693316,\n",
    "# 'eval_recall': 0.7659751512607381,\n",
    "# 'eval_runtime': 53.0067,\n",
    "# 'eval_samples_per_second': 239.536,\n",
    "# 'eval_steps_per_second': 1.887,\n",
    "# 'epoch': 3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.7495931386947632,\n",
       " 'test_accuracy': 0.7519098999763724,\n",
       " 'test_f1': 0.7548794129394598,\n",
       " 'test_precision': 0.7494222713482069,\n",
       " 'test_recall': 0.7691100491384807,\n",
       " 'test_runtime': 53.2919,\n",
       " 'test_samples_per_second': 238.254,\n",
       " 'test_steps_per_second': 1.876,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset = test_dataset, metric_key_prefix='test')\n",
    "#{'test_loss': 0.7234671711921692,\n",
    "# 'test_accuracy': 0.753012522643144,\n",
    "# 'test_f1': 0.7555936292144514,\n",
    "# 'test_precision': 0.7528711923327588,\n",
    "# 'test_recall': 0.7638676917937891,\n",
    "# 'test_runtime': 52.983,\n",
    "# 'test_samples_per_second': 239.643,\n",
    "# 'test_steps_per_second': 1.887,\n",
    "# 'epoch': 3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 173960\n",
      "  Batch size = 128\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.2279217541217804,\n",
       " 'train_accuracy': 0.9255461025523108,\n",
       " 'train_f1': 0.9160572857435375,\n",
       " 'train_precision': 0.9144830092516644,\n",
       " 'train_recall': 0.9178590947226644,\n",
       " 'train_runtime': 733.623,\n",
       " 'train_samples_per_second': 237.125,\n",
       " 'train_steps_per_second': 1.854,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset = train_dataset, metric_key_prefix='train')\n",
    "#{'train_loss': 0.2518060803413391,\n",
    "# 'train_accuracy': 0.9174580363301909,\n",
    "# 'train_f1': 0.9066049910963004,\n",
    "# 'train_precision': 0.9084922960852491,\n",
    "# 'train_recall': 0.9048915818138704,\n",
    "# 'train_runtime': 729.6597,\n",
    "# 'train_samples_per_second': 238.413,\n",
    "# 'train_steps_per_second': 1.864,\n",
    "# 'epoch': 3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in models/BERT_FEVER_v2_model_SPECIAL_CHARS_CODED/config.json\n",
      "Model weights saved in models/BERT_FEVER_v2_model_SPECIAL_CHARS_CODED/pytorch_model.bin\n",
      "tokenizer config file saved in models/BERT_FEVER_v2_tok_SPECIAL_CHARS_CODE/tokenizer_config.json\n",
      "Special tokens file saved in models/BERT_FEVER_v2_tok_SPECIAL_CHARS_CODE/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/BERT_FEVER_v2_tok_SPECIAL_CHARS_CODE/tokenizer_config.json',\n",
       " 'models/BERT_FEVER_v2_tok_SPECIAL_CHARS_CODE/special_tokens_map.json',\n",
       " 'models/BERT_FEVER_v2_tok_SPECIAL_CHARS_CODE/vocab.txt',\n",
       " 'models/BERT_FEVER_v2_tok_SPECIAL_CHARS_CODE/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('models/BERT_FEVER_v2_model_SPECIAL_CHARS_CODED')\n",
    "tokenizer.save_pretrained('models/BERT_FEVER_v2_tok_SPECIAL_CHARS_CODED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['refutes', 'not enough info', 'refutes', 'not enough info'], tensor([[-2.3237,  1.6228,  1.1109],\n",
      "        [-0.3627, -1.9348,  2.2680],\n",
      "        [-2.6840,  2.7349,  0.3152],\n",
      "        [-2.4935, -0.4070,  3.5756]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "classes = ['supports','refutes','not enough info']\n",
    "\n",
    "claims = [\n",
    "    'Barack Obama was part of the 109th United States Congress.',\n",
    "    'France is a member of the European Air Transport Command.',\n",
    "    'The Moon\\'s diocese is the Roman Catholic Diocese of Arizona.',\n",
    "    'Toyota is a member of the Linux Foundation.'\n",
    "]\n",
    "\n",
    "sentences = [\n",
    "    'OBAMA, Barack, a Senator from Illinois and 44th President of the United States;',\n",
    "    'The driving parties were France and Germany, who looked back at a strong bilateral cooperation in the field of air transport.',\n",
    "    'It might sound strange, but in addition to encompassing nine counties and hundreds of cities, the Diocese of Orlando, Florida also has jurisdiction over an otherworldly object: the Moon.',\n",
    "    'Carmakers are using new technologies to deliver on consumer expectations for the same connectivity in their cars as they’ve come to expect in their homes and offices.'\n",
    "]\n",
    "\n",
    "def get_predictions(claims, sentences, soft=False):\n",
    "    enc = tokenizer(claims,sentences,max_length=MAX_LEN,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',).to(DEVICE)\n",
    "\n",
    "    # evaluate model:\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=enc['input_ids'], attention_mask=enc['attention_mask'])\n",
    "        predicted_classes = [classes[i] for i in np.argmax(torch.softmax(out.logits,dim=1).tolist(), axis=1)]\n",
    "        return predicted_classes, torch.softmax(out.logits,dim=1).tolist() if soft else out.logits\n",
    "\n",
    "print(get_predictions(claims, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Linux Foundation is a nonprofit organization.\n",
      "Total: NOT ENOUGH INFO [2.911760091781616, 3.2573153972625732, 13.830924987792969]\n",
      "\n",
      "The Linux Foundation is not a nonprofit organization.\n",
      "Total: NOT ENOUGH INFO [1.6688885688781738, 3.6800355911254883, 14.651076316833496]\n",
      "\n",
      "Toyota is a member of the Linux Foundation.\n",
      "Total: NOT ENOUGH INFO [7.7435383796691895, 0.5286341309547424, 11.727827072143555]\n",
      "\n",
      "Toyota is not a member of the Linux Foundation.\n",
      "Total: REFUTES [0.5312831401824951, 12.267521858215332, 7.20119571685791]\n",
      "\n",
      "Paris is a city in France.\n",
      "Total: REFUTES [3.5600435733795166, 15.016714096069336, 1.4232423305511475]\n",
      "\n",
      "Toyota is the owner of Denso.\n",
      "Total: NOT ENOUGH INFO [0.06672655045986176, 0.6592279076576233, 19.274045944213867]\n",
      "\n",
      "Jim Zemlin is the executive director of the Linux Foundation.\n",
      "Total: NOT ENOUGH INFO [1.2102147340774536, 0.5589517951011658, 18.2308349609375]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "claims = [\n",
    "    'The Linux Foundation is a nonprofit organization.',\n",
    "    'The Linux Foundation is not a nonprofit organization.',\n",
    "    'Toyota is a member of the Linux Foundation.',\n",
    "    'Toyota is not a member of the Linux Foundation.',\n",
    "    'Paris is a city in France.',\n",
    "    'Toyota is the owner of Denso.',\n",
    "    'Jim Zemlin is the executive director of the Linux Foundation.'\n",
    "]\n",
    "# text from https://www.linuxfoundation.org/press-release/2011/07/toyota-joins-linux-foundation/\n",
    "test_text = '''\n",
    "    Toyota looks towards open innovation and collaboration to help transform auto industry\n",
    "\n",
    "    SAN FRANCISCO, July 5, 2011 – The Linux Foundation, the nonprofit organization dedicated to accelerating the growth of Linux, today announced that Toyota is its newest member.\n",
    "\n",
    "    A major shift is underway in the automotive industry. Carmakers are using new technologies to deliver on consumer expectations for the same connectivity in their cars as they’ve come to expect in their homes and offices. From dashboard computing to In-Vehicle-Infotainment (IVI), automobiles are becoming the latest wireless devices – on wheels.\n",
    "\n",
    "    The Linux operating system is providing a common platform that helps connect the world’s network of devices, including cars. As an open source operating system, it provides automakers and their partners the flexibility they require to bring to market the latest technology features quickly.\n",
    "\n",
    "    Toyota is joining The Linux Foundation as a Gold member to maximize its own investment in Linux while fostering open innovation throughout the automotive ecosystem.\n",
    "\n",
    "    “Linux gives us the flexibility and technology maturity we require to evolve our In-Vehicle-Infotainment and communications systems to address the expectations of our customers,” said Kenichi Murata, Project General Manager, Electronics Development Div. 1, TOYOTA MOTOR CORPORATION. “The Linux Foundation provides us with a neutral forum in which we can collaborate with the world’s leading technology companies on open innovation that accelerates that evolution.”\n",
    "\n",
    "    “We are very pleased to welcome Toyota to The Linux Foundation. The company’s leadership and proven innovation will bring important contributions to the advancement of Linux,” said Jim Zemlin, executive director at The Linux Foundation. ‘Toyota’s investment in Linux is a testament to the ubiquity of the operating system and its ability to support the latest market requirements.”\n",
    "\n",
    "    About The Linux Foundation\n",
    "    The Linux Foundation is a nonprofit consortium dedicated to fostering the growth of Linux. Founded in 2000, the organization sponsors the work of Linux creator Linus Torvalds and promotes, protects and advances the Linux operating system by marshaling the resources of its members and the open source development community. The Linux Foundation provides a neutral forum for collaboration and education by hosting Linux conferences, including LinuxCon, and generating original Linux research and content that advances the understanding of the Linux platform. Its web properties, including Linux.com, reach approximately two million people per month and include important Linux video resources. The organization also provides extensive Linux training opportunities that feature the Linux kernel community’s leading experts as instructors. Follow The Linux Foundation on Twitter.\n",
    "\n",
    "    ###\n",
    "\n",
    "    Trademarks: The Linux Foundation, Linux Standard Base, MeeGo and Yocto Project are trademarks of The Linux Foundation. Linux is a trademark of Linus Torvalds.\n",
    "'''\n",
    "for claim in claims[:]:\n",
    "    print(claim)\n",
    "    sents = [s.text.strip() for s in list(nlp(test_text).sents)]\n",
    "    cls, outputs = get_predictions([claim]*len(sents), sents, soft=False)\n",
    "    #for s,c,o in list(zip(sents,cls,outputs)):\n",
    "    #    print(s)\n",
    "    #    print(c.upper())\n",
    "    #    print(o)\n",
    "    #    print('--------------------------------------\\n')\n",
    "    print('Total:',end=' ')\n",
    "    sum_scores = torch.sum(torch.softmax(outputs,dim=1), dim=0).tolist()\n",
    "    print(classes[np.argmax(sum_scores)].upper(), sum_scores)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model again for sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/BERT_FEVER_v2_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file models/BERT_FEVER_v2_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at models/BERT_FEVER_v2_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file models/BERT_FEVER_v2_tok/added_tokens.json. We won't load it.\n",
      "Didn't find file models/BERT_FEVER_v2_tok/tokenizer.json. We won't load it.\n",
      "loading file models/BERT_FEVER_v2_tok/vocab.txt\n",
      "loading file None\n",
      "loading file models/BERT_FEVER_v2_tok/special_tokens_map.json\n",
      "loading file models/BERT_FEVER_v2_tok/tokenizer_config.json\n",
      "loading file None\n"
     ]
    }
   ],
   "source": [
    "model2 = BertForSequenceClassification.from_pretrained('models/BERT_FEVER_v2_model')\n",
    "tokenizer2 = BertTokenizer.from_pretrained('models/BERT_FEVER_v2_tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12697\n",
      "  Batch size = 128\n",
      "/home/k20036346/.conda/envs/cuda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.7234671711921692,\n",
       " 'test_accuracy': 0.753012522643144,\n",
       " 'test_f1': 0.7555936292144514,\n",
       " 'test_precision': 0.7528711923327588,\n",
       " 'test_recall': 0.7638676917937891,\n",
       " 'test_runtime': 53.3606,\n",
       " 'test_samples_per_second': 237.947,\n",
       " 'test_steps_per_second': 1.874}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics      # metrics to be computed\n",
    ")\n",
    "\n",
    "trainer2.evaluate(eval_dataset = test_dataset, metric_key_prefix='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supports\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.0538, -0.3443, -0.8999]], device='cuda:0'), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "classes = ['supports','refutes','not enough info']\n",
    "claim = 'John was born in December.'\n",
    "sentence = 'John had lunch in December of 1992 with his dad.'\n",
    "enc = tokenizer2(claim,sentence,max_length=MAX_LEN,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',).to(DEVICE)\n",
    "\n",
    "# evaluate model:\n",
    "model2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model2(input_ids=enc['input_ids'], attention_mask=enc['attention_mask'])\n",
    "    print(classes[np.argmax(torch.softmax(out.logits,dim=1).tolist(), axis=1)[0]])\n",
    "    print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.eval() is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn off them during model evaluation, and .eval() will do it for you. In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation:\n",
    "\n",
    "```\n",
    "# evaluate model:\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ...\n",
    "    out_data = model(data)\n",
    "    ...\n",
    "```\n",
    "\n",
    "BUT, don't forget to turn back to training mode after eval step:\n",
    "\n",
    "```\n",
    "# training step\n",
    "...\n",
    "model.train()\n",
    "...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
